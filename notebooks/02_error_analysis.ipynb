{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130cf19b",
   "metadata": {},
   "source": [
    "# 2 — Error Analysis & Training Dynamics\n",
    "\n",
    "Analisi degli errori di BERTino sul test set, confronto con la baseline, e visualizzazione delle dinamiche di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39ee54",
   "metadata": {},
   "source": [
    "## 2.1 — Predizioni su test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica test set\n",
    "ds_test = load_dataset(\"nlp-unibo/AMELIA\", split=\"test\")\n",
    "test_df = ds_test.to_pandas()\n",
    "test_df[\"text_len_words\"] = test_df[\"Text\"].str.split().str.len()\n",
    "\n",
    "# Etichette gold\n",
    "LABEL2ID = {\"prem\": 0, \"conc\": 1}\n",
    "ID2LABEL = {0: \"prem\", 1: \"conc\"}\n",
    "test_df[\"gold_id\"] = test_df[\"Component\"].map(LABEL2ID)\n",
    "\n",
    "print(f\"Test set: {len(test_df)} campioni\")\n",
    "print(test_df[\"Component\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0139666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni baseline (TF-IDF + LR)\n",
    "from amelia_experiment.baseline import load_baseline_pipeline\n",
    "from amelia_experiment.preprocess import normalize_text\n",
    "\n",
    "baseline_pipeline = load_baseline_pipeline()\n",
    "baseline_texts = [normalize_text(t) for t in test_df[\"Text\"]]\n",
    "test_df[\"baseline_pred\"] = baseline_pipeline.predict(baseline_texts)\n",
    "test_df[\"baseline_label\"] = test_df[\"baseline_pred\"].map(ID2LABEL)\n",
    "test_df[\"baseline_correct\"] = test_df[\"gold_id\"] == test_df[\"baseline_pred\"]\n",
    "\n",
    "print(\n",
    "    f\"Baseline: {test_df['baseline_correct'].sum()}/{len(test_df)} corretti\"\n",
    "    f\" ({test_df['baseline_correct'].mean() * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c47492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni BERTino\n",
    "from amelia_experiment.bertino import load_bertino, predict_texts\n",
    "\n",
    "model, tokenizer = load_bertino()\n",
    "bertino_preds = predict_texts(test_df[\"Text\"].tolist(), model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_df[\"bertino_pred\"] = [p[\"label_id\"] for p in bertino_preds]\n",
    "test_df[\"bertino_label\"] = [p[\"label\"] for p in bertino_preds]\n",
    "test_df[\"bertino_confidence\"] = [p[\"confidence\"] for p in bertino_preds]\n",
    "test_df[\"bertino_correct\"] = test_df[\"gold_id\"] == test_df[\"bertino_pred\"]\n",
    "\n",
    "print(\n",
    "    f\"BERTino:  {test_df['bertino_correct'].sum()}/{len(test_df)} corretti\"\n",
    "    f\" ({test_df['bertino_correct'].mean() * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639ef58",
   "metadata": {},
   "source": [
    "## 2.2 — Analisi degli errori di BERTino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_bert = test_df[~test_df[\"bertino_correct\"]].copy()\n",
    "\n",
    "print(f\"Totale errori BERTino: {len(errors_bert)}\")\n",
    "print(\"\\nTipologia errori:\")\n",
    "error_types = errors_bert.groupby([\"Component\", \"bertino_label\"]).size().reset_index(name=\"count\")\n",
    "print(error_types.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Errori prem→conc (False Positive per conc) ---\")\n",
    "fp = errors_bert[errors_bert[\"Component\"] == \"prem\"]\n",
    "print(f\"N={len(fp)}, confidence media: {fp['bertino_confidence'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n--- Errori conc→prem (False Negative per conc) ---\")\n",
    "fn = errors_bert[errors_bert[\"Component\"] == \"conc\"]\n",
    "print(f\"N={len(fn)}, confidence media: {fn['bertino_confidence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80587287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra i testi degli errori con il testo troncato\n",
    "print(\"=== ERRORI DI BERTINO SUL TEST SET ===\")\n",
    "print()\n",
    "for i, (_, row) in enumerate(errors_bert.iterrows(), 1):\n",
    "    text_preview = row[\"Text\"][:200] + \"...\" if len(row[\"Text\"]) > 200 else row[\"Text\"]\n",
    "    print(\n",
    "        f\"[{i}] Gold: {row['Component']} → Pred: {row['bertino_label']} \"\n",
    "        f\"(conf: {row['bertino_confidence']:.3f}, \"\n",
    "        f\"len: {row['text_len_words']}w)\"\n",
    "    )\n",
    "    print(f\"    {text_preview}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcbcd5",
   "metadata": {},
   "source": [
    "## 2.3 — Confidence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribuzione confidence corretti vs errori\n",
    "correct = test_df[test_df[\"bertino_correct\"]]\n",
    "wrong = test_df[~test_df[\"bertino_correct\"]]\n",
    "\n",
    "axes[0].hist(\n",
    "    correct[\"bertino_confidence\"],\n",
    "    bins=30,\n",
    "    alpha=0.6,\n",
    "    label=f\"Corretti (n={len(correct)})\",\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "axes[0].hist(\n",
    "    wrong[\"bertino_confidence\"],\n",
    "    bins=15,\n",
    "    alpha=0.7,\n",
    "    label=f\"Errori (n={len(wrong)})\",\n",
    "    color=\"#C44E52\",\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Confidence\")\n",
    "axes[0].set_ylabel(\"Frequenza\")\n",
    "axes[0].set_title(\"Distribuzione confidence — BERTino\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence vs lunghezza testo (errori evidenziati)\n",
    "axes[1].scatter(\n",
    "    correct[\"text_len_words\"],\n",
    "    correct[\"bertino_confidence\"],\n",
    "    alpha=0.2,\n",
    "    s=15,\n",
    "    color=\"#4C72B0\",\n",
    "    label=\"Corretti\",\n",
    ")\n",
    "axes[1].scatter(\n",
    "    wrong[\"text_len_words\"],\n",
    "    wrong[\"bertino_confidence\"],\n",
    "    alpha=0.9,\n",
    "    s=40,\n",
    "    color=\"#C44E52\",\n",
    "    marker=\"x\",\n",
    "    linewidths=2,\n",
    "    label=\"Errori\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Lunghezza testo (parole)\")\n",
    "axes[1].set_ylabel(\"Confidence\")\n",
    "axes[1].set_title(\"Confidence vs Lunghezza — Errori evidenziati\")\n",
    "axes[1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../results/plots/confidence_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confidence media — corretti: {correct['bertino_confidence'].mean():.4f}\")\n",
    "print(f\"Confidence media — errori:   {wrong['bertino_confidence'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fcc1d",
   "metadata": {},
   "source": [
    "## 2.4 — Confronto errori: Baseline vs BERTino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errori della baseline\n",
    "errors_base = test_df[~test_df[\"baseline_correct\"]]\n",
    "errors_bert_idx = set(errors_bert.index)\n",
    "errors_base_idx = set(errors_base.index)\n",
    "\n",
    "both_wrong = errors_bert_idx & errors_base_idx\n",
    "only_bert_wrong = errors_bert_idx - errors_base_idx\n",
    "only_base_wrong = errors_base_idx - errors_bert_idx\n",
    "\n",
    "print(f\"Errori solo baseline:             {len(only_base_wrong)}\")\n",
    "print(f\"Errori solo BERTino:              {len(only_bert_wrong)}\")\n",
    "print(f\"Errori in comune:                 {len(both_wrong)}\")\n",
    "print(\n",
    "    f\"\\nBaseline sbaglia {len(errors_base_idx)} campioni, BERTino ne sbaglia {len(errors_bert_idx)}.\"\n",
    ")\n",
    "print(f\"BERTino corregge {len(only_base_wrong)} errori della baseline \")\n",
    "print(f\"ma ne introduce {len(only_bert_wrong)} nuovi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: correttezza incrociata\n",
    "cross = pd.crosstab(\n",
    "    test_df[\"baseline_correct\"].map({True: \"Base ✓\", False: \"Base ✗\"}),\n",
    "    test_df[\"bertino_correct\"].map({True: \"BERT ✓\", False: \"BERT ✗\"}),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    cross, annot=True, fmt=\"d\", cmap=\"YlOrRd_r\", ax=ax, cbar=False, linewidths=2, linecolor=\"white\"\n",
    ")\n",
    "ax.set_title(\"Correttezza incrociata — Baseline vs BERTino\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../results/plots/cross_correctness.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2df62",
   "metadata": {},
   "source": [
    "## 2.5 — Lunghezza dei testi mal classificati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lunghezza media (parole):\")\n",
    "print(f\"  Globale test set:          {test_df['text_len_words'].mean():.1f}\")\n",
    "print(f\"  Errori BERTino:            {errors_bert['text_len_words'].mean():.1f}\")\n",
    "print(f\"  Errori Baseline:           {errors_base['text_len_words'].mean():.1f}\")\n",
    "print(f\"  Corretti BERTino:          {correct['text_len_words'].mean():.1f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "data_to_plot = [\n",
    "    correct[\"text_len_words\"].values,\n",
    "    errors_bert[\"text_len_words\"].values,\n",
    "    errors_base[\"text_len_words\"].values,\n",
    "]\n",
    "labels_to_plot = [\n",
    "    f\"BERTino corretti (n={len(correct)})\",\n",
    "    f\"BERTino errori (n={len(errors_bert)})\",\n",
    "    f\"Baseline errori (n={len(errors_base)})\",\n",
    "]\n",
    "bp = ax.boxplot(\n",
    "    data_to_plot,\n",
    "    labels=labels_to_plot,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"#4C72B0\", alpha=0.5),\n",
    ")\n",
    "bp[\"boxes\"][1].set_facecolor(\"#C44E52\")\n",
    "bp[\"boxes\"][2].set_facecolor(\"#DD8452\")\n",
    "ax.set_ylabel(\"Lunghezza (parole)\")\n",
    "ax.set_title(\"Lunghezza testi: corretti vs errori\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../results/plots/error_length_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db06043",
   "metadata": {},
   "source": [
    "## 2.6 — Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica training history (se disponibile)\n",
    "history_path = RESULTS_DIR / \"metrics\" / \"bertino_training_history.json\"\n",
    "\n",
    "if history_path.exists():\n",
    "    with open(history_path) as f:\n",
    "        history = json.load(f)[\"log_history\"]\n",
    "\n",
    "    # Separa training logs (con 'loss') e eval logs (con 'eval_loss')\n",
    "    train_logs = [h for h in history if \"loss\" in h and \"eval_loss\" not in h]\n",
    "    eval_logs = [h for h in history if \"eval_macro_f1\" in h]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Training loss\n",
    "    if train_logs:\n",
    "        steps = [h[\"step\"] for h in train_logs]\n",
    "        losses = [h[\"loss\"] for h in train_logs]\n",
    "        axes[0].plot(steps, losses, color=\"#4C72B0\", linewidth=2)\n",
    "        axes[0].set_xlabel(\"Step\")\n",
    "        axes[0].set_ylabel(\"Training Loss\")\n",
    "        axes[0].set_title(\"Training Loss nel tempo\")\n",
    "\n",
    "    # Eval Macro-F1 per epoca\n",
    "    if eval_logs:\n",
    "        epochs = [h.get(\"epoch\", i + 1) for i, h in enumerate(eval_logs)]\n",
    "        f1s = [h[\"eval_macro_f1\"] for h in eval_logs]\n",
    "        accs = [h.get(\"eval_accuracy\", 0) for h in eval_logs]\n",
    "\n",
    "        axes[1].plot(\n",
    "            epochs, f1s, \"o-\", color=\"#4C72B0\", linewidth=2, markersize=8, label=\"Macro-F1\"\n",
    "        )\n",
    "        axes[1].plot(\n",
    "            epochs, accs, \"s--\", color=\"#DD8452\", linewidth=2, markersize=8, label=\"Accuracy\"\n",
    "        )\n",
    "        axes[1].set_xlabel(\"Epoca\")\n",
    "        axes[1].set_ylabel(\"Score\")\n",
    "        axes[1].set_title(\"Eval Macro-F1 & Accuracy per epoca\")\n",
    "        axes[1].legend()\n",
    "        axes[1].set_ylim(0.8, 1.0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"../results/plots/training_dynamics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Log entries:\", len(history))\n",
    "    if eval_logs:\n",
    "        print(f\"\\nMacro-F1 per epoca: {[f'{f:.4f}' for f in f1s]}\")\n",
    "else:\n",
    "    print(f\"⚠ Training history non trovata in {history_path}\")\n",
    "    print(\n",
    "        \"  Riesegui train_bert.py per generarla (la versione aggiornata la salva automaticamente).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8645c",
   "metadata": {},
   "source": [
    "## 2.7 — Mini Ablation Study\n",
    "\n",
    "Confronto dei risultati con diverse configurazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation 1: Effetto degli n-grammi sulla baseline TF-IDF + LR\n",
    "from amelia_experiment.baseline import train_tfidf_lr\n",
    "from amelia_experiment.dataset import load_amelia\n",
    "from amelia_experiment.metrics import classification_dict\n",
    "from amelia_experiment.preprocess import encode_label, normalize_text\n",
    "\n",
    "train_ds = load_amelia(split=\"train\")\n",
    "val_ds = load_amelia(split=\"validation\")\n",
    "test_ds = load_amelia(split=\"test\")\n",
    "\n",
    "# Prepara test labels\n",
    "test_texts = [normalize_text(t) for t in test_ds[\"Text\"]]\n",
    "test_labels = [encode_label(lab) for lab in test_ds[\"Component\"]]\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for ngram in [(1, 1), (1, 2), (1, 3), (2, 2)]:\n",
    "    for max_feat in [10_000, 50_000]:\n",
    "        result = train_tfidf_lr(\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            ngram_range=ngram,\n",
    "            max_features=max_feat,\n",
    "        )\n",
    "        # Valuta su test\n",
    "        pipeline = result[\"pipeline\"]\n",
    "        test_preds = pipeline.predict(test_texts).tolist()\n",
    "        test_metrics = classification_dict(test_labels, test_preds)\n",
    "\n",
    "        ablation_results.append(\n",
    "            {\n",
    "                \"ngram_range\": str(ngram),\n",
    "                \"max_features\": max_feat,\n",
    "                \"val_macro_f1\": result[\"val_metrics\"][\"macro_f1\"],\n",
    "                \"test_macro_f1\": test_metrics[\"macro_f1\"],\n",
    "                \"test_accuracy\": test_metrics[\"accuracy\"],\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"  ngram={ngram}, max_feat={max_feat:>6d} → \"\n",
    "            f\"val F1={result['val_metrics']['macro_f1']:.4f}, \"\n",
    "            f\"test F1={test_metrics['macro_f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "print(\"\\n=== Tabella Ablation Baseline ===\")\n",
    "print(ablation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione ablation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = range(len(ablation_df))\n",
    "labels = [f\"{r['ngram_range']}\\nk={r['max_features'] // 1000}k\" for _, r in ablation_df.iterrows()]\n",
    "\n",
    "bars_val = ax.bar(\n",
    "    [i - 0.15 for i in x],\n",
    "    ablation_df[\"val_macro_f1\"],\n",
    "    0.3,\n",
    "    label=\"Validation F1\",\n",
    "    color=\"#4C72B0\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars_test = ax.bar(\n",
    "    [i + 0.15 for i in x],\n",
    "    ablation_df[\"test_macro_f1\"],\n",
    "    0.3,\n",
    "    label=\"Test F1\",\n",
    "    color=\"#DD8452\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel(\"Configurazione (ngram_range, max_features)\")\n",
    "ax.set_ylabel(\"Macro-F1\")\n",
    "ax.set_title(\"Ablation Study — TF-IDF + Logistic Regression\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.6, 0.85)\n",
    "\n",
    "# Linea BERTino come riferimento\n",
    "ax.axhline(y=0.9221, color=\"#55A868\", linestyle=\"--\", linewidth=2, label=\"BERTino test F1\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../results/plots/ablation_baseline.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Miglior configurazione baseline\n",
    "best = ablation_df.loc[ablation_df[\"test_macro_f1\"].idxmax()]\n",
    "print(f\"\\nMiglior baseline: ngram={best['ngram_range']}, max_features={int(best['max_features'])}\")\n",
    "print(f\"  Test Macro-F1: {best['test_macro_f1']:.4f}\")\n",
    "print(f\"  Gap con BERTino: {0.9221 - best['test_macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dc231",
   "metadata": {},
   "source": [
    "## 2.8 — Riepilogo Error Analysis\n",
    "\n",
    "**Conclusioni chiave:**\n",
    "\n",
    "1. **Errori di BERTino:** pochi e con confidence inferiore alla media → il modello è ben calibrato\n",
    "2. **Errori in comune con la baseline:** sono i campioni intrinsecamente più ambigui del dataset\n",
    "3. **BERTino corregge molti errori della baseline,** dimostrando che le rappresentazioni contestuali catturano segnali che BoW non vede\n",
    "4. **Training dynamics:** la loss decresce regolarmente, il Macro-F1 si stabilizza dopo la 2ª epoca — 3 epoche sono un buon compromesso\n",
    "5. **Ablation baseline:** la configurazione (1,2) con 50k features è tra le migliori, ma nessuna variante si avvicina a BERTino"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
