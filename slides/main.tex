% !TEX program = pdflatex
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{amsfonts,amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{multirow}

\usetheme{sintef}
\usefonttheme[onlymath]{serif}

% ── Background title ─────────────────────────────────────────────────────────
\titlebackground*{assets/background}

% ── Colored href helper ──────────────────────────────────────────────────────
\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

% ── Metadata ─────────────────────────────────────────────────────────────────
\title{Classificazione Argomentativa\\su Testi Legali Italiani}
\subtitle{AMELIA + BERTino --- Premessa vs Conclusione}
\author{\href{mailto:orso.peruzzi@stud.unifi.it}{Orso Peruzzi}}
\course{Text Mining \& NLP}
\date{A.A.\ 2024/2025}

\begin{document}

% ══════════════════════════════════════════════════════════════════════════════
% TITLE
% ══════════════════════════════════════════════════════════════════════════════
\maketitle

% ══════════════════════════════════════════════════════════════════════════════
% TABLE OF CONTENTS
% ══════════════════════════════════════════════════════════════════════════════
\footlinecolor{maincolor}
\begin{frame}{Indice}
  \begin{multicols}{2}
    \tableofcontents
  \end{multicols}
\end{frame}

% ══════════════════════════════════════════════════════════════════════════════
%                           1. INTRODUZIONE
% ══════════════════════════════════════════════════════════════════════════════
\begin{chapter}[assets/background_negative]{maincolor}{Introduzione}
\end{chapter}
\section{Introduzione}

% ── Slide 1: Motivazione ─────────────────────────────────────────────────────
\footlinecolor{maincolor}
\begin{frame}{Motivazione}
  \begin{itemize}
    \item \textbf{Argument Mining}: estrarre e classificare automaticamente
          componenti argomentative da testi~\cite{lippi2016argumentation}.
    \item Applicazione al \textbf{dominio legale italiano}: supporto decisionale
          per giuristi, automazione dell'analisi documentale.
    \item \textbf{Sfide specifiche}: periodi lunghi, terminologia tecnica,
          struttura argomentativa implicita, \textbf{lingua italiana}
          (risorse NLP limitate rispetto all'inglese).
  \end{itemize}

  \vspace{0.3cm}
  \begin{colorblock}[white]{maincolor}{Domanda di ricerca}
    Un Transformer pre-addestrato in italiano (\textbf{BERTino}) supera
    una pipeline classica \mbox{TF-IDF + Logistic Regression}
    nel classificare premesse e conclusioni in testi giuridici?
  \end{colorblock}
\end{frame}

% ── Slide 2: Collegamento con il Corso ───────────────────────────────────────
\begin{frame}{Collegamento con il Corso}
  \begin{itemize}
    \item A lezione abbiamo seguito il percorso:
          preprocessing $\to$ BoW $\to$ \textbf{TF-IDF} $\to$ Word2Vec
          $\to$ classificazione con \texttt{LogisticRegression} (es.\ LexGLUE).
    \item \textbf{Questo progetto} applica la stessa pipeline a un dataset legale italiano
          e la confronta con un approccio neurale:
    \begin{enumerate}
      \item \textbf{Baseline}: TF-IDF + LR (approccio visto in classe)
      \item \textbf{Estensione}: fine-tuning di \textbf{BERTino}~\cite{bertino2020},
            un DistilBERT~\cite{sanh2019distilbert} italiano
            basato su BERT~\cite{devlin2019bert}
    \end{enumerate}
    \item \textbf{Obiettivo}: confronto quantitativo tra rappresentazioni
          \emph{bag-of-words} (senza contesto) e rappresentazioni
          \emph{contestuali} (Transformer)
  \end{itemize}

  \vspace{0.2cm}
  {\small $\Rightarrow$ Il progetto risponde alla domanda fondamentale del corso:
  \textbf{quanto conta il contesto} nella comprensione del linguaggio?}
\end{frame}

% ── Slide 3: Da BoW a Transformer ───────────────────────────────────────────
\begin{frame}{Da Bag-of-Words a Transformer --- Evoluzione degli Approcci}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Approccio} & \textbf{Contesto?} & \textbf{Pre-training?} & \textbf{Usato qui} \\
    \midrule
    BoW / TF-IDF             & No  & No  & \checkmark\ Baseline \\
    Word2Vec                 & Locale (finestra) & S\`{i}  & --- \\
    BERT~\cite{devlin2019bert} & Bidirezionale & S\`{i}  & --- \\
    DistilBERT~\cite{sanh2019distilbert} & Bidirezionale & S\`{i} (distillato) & --- \\
    \textbf{BERTino}~\cite{bertino2020} & \textbf{Bidirezionale} & \textbf{S\`{i} (italiano)} & \checkmark \\
    \bottomrule
  \end{tabular}

  \vspace{0.4cm}
  {\small
  \textbf{BERTino} = DistilBERT addestrato su corpora italiani.
  6 layer, 768 hidden units, \textbf{66M parametri}.\\
  Rispetto a BERT-base (110M): $\sim$40\% pi\`{u} leggero, $\sim$60\% pi\`{u} veloce,
  prestazioni competitive.\\[0.15cm]
  $\Rightarrow$ \emph{Ipotesi}: le rappresentazioni contestuali catturano la struttura
  argomentativa che TF-IDF non pu\`{o} modellare.}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                           2. DATASET
% ══════════════════════════════════════════════════════════════════════════════
\begin{chapter}[assets/background_negative]{sintefred}{Dataset AMELIA}
\end{chapter}
\section{Dataset}

% ── Slide 4: Dataset AMELIA ──────────────────────────────────────────────────
\footlinecolor{sintefred}
\begin{frame}{Dataset AMELIA --- Task 1}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \begin{itemize}
        \item \textbf{AMELIA}: Argument Mining Evaluation on Legal documents
              in ItAlian~\cite{amelia2024}.
        \item Decisioni della \textbf{Commissione Tributaria} (materia IVA).
        \item Task 1: classificazione binaria di ciascuna componente
              argomentativa come \texttt{prem} (premessa) o \texttt{conc} (conclusione).
        \item Split predefiniti dagli autori (no cross-validation necessaria).
        \item Licenza: \textbf{CC BY 4.0}.
      \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
      \centering
      \begin{tabular}{lrr}
        \toprule
        \textbf{Split} & \textbf{N} & \textbf{prem\,:\,conc} \\
        \midrule
        Train      & 2\,108 & 7.7\,:\,1 \\
        Validation & 609    & 6.5\,:\,1 \\
        Test       & 594    & 6.6\,:\,1 \\
        \midrule
        \textbf{Totale} & \textbf{3\,311} & 7.3\,:\,1 \\
        \bottomrule
      \end{tabular}
      \vspace{0.3cm}
      \\{\small $\sim$87\% premesse, $\sim$13\% conclusioni}
    \end{column}
  \end{columns}

  \vspace{0.2cm}
  {\small $\Rightarrow$ Lo sbilanciamento \`{e} molto marcato. Prima di procedere,
  analizziamo i dati per capire \textbf{quali sfide} questo comporta.}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                       3. EXPLORATORY DATA ANALYSIS
% ══════════════════════════════════════════════════════════════════════════════
\section{Exploratory Data Analysis}

% ── Slide 5: Distribuzione classi ────────────────────────────────────────────
\begin{frame}{EDA --- Distribuzione delle Classi}
  \centering
  \includegraphics[width=0.82\textwidth]{../results/plots/class_distribution.png}

  \vspace{0.2cm}
  {\small Il rapporto premesse/conclusioni \`{e} costante nei tre split ($\sim$7:1),
  confermando che la partizione \`{e} stratificata.\\[0.1cm]
  \textbf{Implicazione}: un modello ``banale'' che predice sempre \texttt{prem}
  otterrebbe accuracy $\sim$87\%, ma Macro-F1 $\sim$0.47 (inutile).\\
  $\Rightarrow$ Adottiamo \textbf{Macro-F1} come metrica principale:
  media aritmetica degli F1 per classe, che penalizza
  modelli che ignorano la classe minoritaria.}
\end{frame}

% ── Slide 6: Lunghezza testi ─────────────────────────────────────────────────
\begin{frame}{EDA --- Distribuzione della Lunghezza dei Testi}
  \begin{columns}
    \begin{column}{0.58\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/text_length_distribution.png}
    \end{column}
    \begin{column}{0.4\textwidth}
      \begin{block}{Statistiche (parole)}
        \centering
        \begin{tabular}{lr}
          \toprule
                    & $\mu$ \\
          \midrule
          prem      & 52.3 \\
          conc      & 17.1 \\
          \bottomrule
        \end{tabular}
      \end{block}
      \vspace{0.15cm}
      {\small Le premesse sono \textbf{3$\times$ pi\`{u} lunghe} delle conclusioni.
      La lunghezza \`{e} un segnale discriminante, ma non sufficiente:
      molti testi si sovrappongono nella zona 10--30 parole.}

      \vspace{0.15cm}
      {\small $\Rightarrow$ Servono feature \emph{semantiche}, non solo statistiche
      sulla lunghezza.}
    \end{column}
  \end{columns}
\end{frame}

% ── Slide 7: Tokenizzazione WordPiece ────────────────────────────────────────
\begin{frame}{EDA --- Analisi della Tokenizzazione WordPiece}
  \centering
  \includegraphics[width=0.82\textwidth]{../results/plots/token_length_distribution.png}

  \vspace{0.2cm}
  {\small La tokenizzazione di BERTino produce in media \textbf{76 token} per premessa
  e \textbf{27 token} per conclusione.\\[0.1cm]
  Solo \textbf{1.2\%} dei campioni (40/3\,311) supera \texttt{max\_length=256}
  $\Rightarrow$ il troncamento \`{e} trascurabile e la scelta di 256 token \`{e} adeguata.\\[0.1cm]
  \textbf{Nota}: se avessimo scelto 128, il 5.8\% sarebbe stato troncato,
  perdendo potenzialmente informazioni nelle premesse pi\`{u} lunghe.}
\end{frame}

% ── Slide 8: TF-IDF ─────────────────────────────────────────────────────────
\begin{frame}{EDA --- Termini Discriminanti (Top-20 TF-IDF)}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/top_tfidf_terms.png}
    \end{column}
    \begin{column}{0.43\textwidth}
      \begin{block}{Osservazioni}
        \begin{itemize}
          \item \texttt{conc}: termini \emph{performativi}
                --- \emph{``appello''},
                \emph{``fondato''}, \emph{``respinto''},
                \emph{``pertanto''}
          \item \texttt{prem}: dominato da \emph{parole funzionali}
                generiche
                (\emph{``di''}, \emph{``la''}, \emph{``in''})
        \end{itemize}
      \end{block}
      {\small Le conclusioni hanno un \textbf{vocabolario specifico}
      (linguaggio dispositivo), le premesse no.\\[0.1cm]
      Ma l'overlap lessicale \`{e} solo \textbf{7.3\%}
      $\Rightarrow$ le due classi condividono quasi tutto il vocabolario.}
    \end{column}
  \end{columns}

  \vspace{0.15cm}
  {\small $\Rightarrow$ Un modello BoW cattura i segnali lessicali forti (``pertanto''),
  ma non pu\`{o} disambiguare i contesti in cui le stesse parole appaiono
  in entrambe le classi.}
\end{frame}

% ── Slide 9: Word Cloud ─────────────────────────────────────────────────────
\begin{frame}{EDA --- Word Cloud (senza stopwords)}
  \centering
  \includegraphics[width=0.88\textwidth]{../results/plots/wordclouds.png}

  \vspace{0.2cm}
  {\small Rimosse le stopwords italiane (articoli, preposizioni, congiunzioni,
  pronomi, ausiliari) per evidenziare i termini \emph{contenutistici}.\\[0.1cm]
  \textbf{Premesse}: \emph{contribuente}, \emph{operazioni}, \emph{IVA},
  \emph{societ\`{a}}, \emph{Corte} --- linguaggio \textbf{fattuale}.\\
  \textbf{Conclusioni}: \emph{appello}, \emph{infondato}, \emph{fondato},
  \emph{sentenza}, \emph{accoglimento} --- linguaggio \textbf{dispositivo}.\\[0.1cm]
  $\Rightarrow$ Le due classi usano registri linguistici diversi, ma
  la differenza \`{e} di \emph{ruolo pragmatico}, non solo lessicale.}
\end{frame}

% ── Slide 10: Dall'EDA alla Metodologia ──────────────────────────────────────
\begin{frame}{Dall'EDA alla Metodologia}
  L'analisi esplorativa ci ha fornito indicazioni concrete per
  le scelte sperimentali:

  \vspace{0.2cm}
  \begin{tabular}{p{5.5cm}p{5.5cm}}
    \toprule
    \textbf{Osservazione EDA} & \textbf{Scelta metodologica} \\
    \midrule
    Sbilanciamento 7:1 & Metrica: \textbf{Macro-F1} (non accuracy) \\
    \addlinespace
    Solo 1.2\% troncato a 256 & \texttt{max\_length = 256} per BERTino \\
    \addlinespace
    Premesse 3$\times$ pi\`{u} lunghe & Nessun taglio artificiale dei testi \\
    \addlinespace
    Overlap lessicale 7.3\% & Servono \textbf{rappresentazioni contestuali}
                              (non solo BoW) \\
    \addlinespace
    Vocabolario specialistico & No stemming (perderebbe informazione) \\
    \bottomrule
  \end{tabular}

  \vspace{0.2cm}
  {\small $\Rightarrow$ Ogni decisione metodologica \`{e} \textbf{motivata dai dati},
  non da scelte arbitrarie.}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                           4. METODOLOGIA
% ══════════════════════════════════════════════════════════════════════════════
\begin{chapter}[assets/background_negative]{sintefblu2}{Metodologia}
\end{chapter}
\section{Metodologia}

% ── Slide 11: Setup Sperimentale ─────────────────────────────────────────────
\footlinecolor{sintefblu2}
\begin{frame}{Setup Sperimentale}
  \begin{itemize}
    \item \textbf{Metrica}: Macro-F1 (motivata dall'EDA: sbilanciamento 7:1)
          \begin{itemize}
            \item $\text{Macro-F1} = \frac{1}{|C|} \sum_{c \in C} F_1^{(c)}
                   \;,\quad F_1 = \frac{2 P R}{P + R}$
            \item Penalizza modelli che ignorano la classe minoritaria
          \end{itemize}
    \item \textbf{Split}: train (2\,108) / validation (609) / test (594) ---
          predefiniti dal dataset, nessun data leakage
    \item \textbf{Riproducibilit\`{a}}: seed = 42, codice versionato, CI automatica
    \item \textbf{Modelli confrontati}:
    \begin{enumerate}
      \item TF-IDF + Logistic Regression (\emph{approccio classico})
      \item BERTino fine-tuned (\emph{Transformer encoder-only})
    \end{enumerate}
    \item \textbf{Framework}: scikit-learn~\cite{sklearn2011},
          Hugging Face Transformers, PyTorch (CUDA)
  \end{itemize}
\end{frame}

% ── Slide 12: Baseline ──────────────────────────────────────────────────────
\begin{frame}{Baseline: TF-IDF + Logistic Regression}
  \begin{itemize}
    \item Pipeline identica a quella vista a lezione (cfr.\ LexGLUE)~\cite{sklearn2011}:
    \begin{enumerate}
      \item \textbf{Preprocessing}: solo normalizzazione whitespace.
            \emph{No stemming}: i testi legali italiani perdono informazione
            con stem aggressivi (es.\ ``fondato'' $\to$ ``fond'').
      \item \textbf{TfidfVectorizer}: unigrammi+bigrammi,
            \texttt{sublinear\_tf=True}, \texttt{min\_df=2},
            \texttt{max\_features=50\,000}.
      \item \textbf{LogisticRegression}: solver LBFGS,
            \texttt{C=1.0}, \texttt{max\_iter=1000}.
    \end{enumerate}
    \item Tempo di addestramento: $< 1$ minuto su CPU.
    \item \textbf{Razionale}: rappresenta il \emph{ceiling} di un approccio
          BoW + classificatore lineare su questo task.
  \end{itemize}

  \vspace{0.15cm}
  {\small $\Rightarrow$ La baseline ci dice \textbf{quanto lontano si pu\`{o} arrivare}
  senza rappresentazioni contestuali.}
\end{frame}

% ── Slide 13: BERTino ───────────────────────────────────────────────────────
\begin{frame}{BERTino: Architettura e Fine-Tuning}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \begin{itemize}
        \item \textbf{BERTino} (\texttt{indigo-ai/BERTino})~\cite{bertino2020}:
              DistilBERT pre-addestrato su corpora italiani.
        \item Architettura BERT~\cite{devlin2019bert} distillata:
              \textbf{6 layer}, 768 hidden, 66M parametri.
        \item Fine-tuning end-to-end con \texttt{Trainer} API:
        \begin{itemize}
          \item Selezione best checkpoint via Macro-F1 su validation
          \item Warmup lineare + weight decay
        \end{itemize}
        \item Batch inference (\texttt{batch\_size=32}).
      \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
      \centering
      \begin{tabular}{ll}
        \toprule
        \textbf{Iperpar.} & \textbf{Valore} \\
        \midrule
        Epoche        & 3 \\
        Train batch   & 16 \\
        Eval batch    & 32 \\
        Learning rate & $2 \times 10^{-5}$ \\
        Weight decay  & 0.01 \\
        Warmup ratio  & 10\% \\
        Max length    & 256 token \\
        Scheduler     & Lineare \\
        \bottomrule
      \end{tabular}
      \vspace{0.25cm}
      \\{\small GPU: NVIDIA RTX 2000 Ada}
      \\{\small Tempo: $\sim$4 minuti}
    \end{column}
  \end{columns}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                           5. RISULTATI
% ══════════════════════════════════════════════════════════════════════════════
\begin{chapter}[assets/background_negative]{maincolor}{Risultati}
\end{chapter}
\section{Risultati}

% ── Slide 14: Risultati Principali ───────────────────────────────────────────
\footlinecolor{maincolor}
\begin{frame}{Risultati Principali}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Modello} & \textbf{Val Macro-F1} & \textbf{Test Macro-F1} & \textbf{Test Accuracy} \\
    \midrule
    TF-IDF + LR          & 0.598 & 0.748 & 91.75\% \\
    \textbf{BERTino}     & \textbf{0.907} & \textbf{0.922} & \textbf{96.46\%} \\
    \midrule
    $\Delta$             & +0.309 & \textbf{+0.174} & +4.71 pp \\
    \bottomrule
  \end{tabular}

  \vspace{0.3cm}
  \begin{colorblock}[white]{maincolor}{Risultato chiave}
    BERTino migliora la Macro-F1 di \textbf{+17.4 punti percentuali} sul test set.\\
    Il guadagno \`{e} concentrato sulla classe minoritaria (\texttt{conc}).
  \end{colorblock}

  \vspace{0.15cm}
  {\small \textbf{Nota}: la baseline ha Val-F1 (0.598) molto pi\`{u} basso del Test-F1 (0.748):
  prestazioni instabili.
  BERTino \`{e} stabile tra val (0.907) e test (0.922)
  $\Rightarrow$ migliore \textbf{generalizzazione}.}
\end{frame}

% ── Slide 15: Analisi per classe ─────────────────────────────────────────────
\begin{frame}{Analisi Per-Classe --- Dove Si Gioca la Partita}
  \centering
  {\small
  \begin{tabular}{ll cccc}
    \toprule
    \textbf{Modello} & \textbf{Classe} & \textbf{Precision} & \textbf{Recall}
      & \textbf{F1} & \textbf{Supporto} \\
    \midrule
    \multirow{2}{*}{TF-IDF + LR}
      & prem & 0.913 & \textbf{1.000} & 0.955 & 516 \\
      & conc & \textbf{1.000} & 0.372 & 0.542 & 78 \\
    \addlinespace
    \multirow{2}{*}{\textbf{BERTino}}
      & prem & 0.979 & 0.981 & \textbf{0.980} & 516 \\
      & conc & 0.870 & \textbf{0.859} & \textbf{0.865} & 78 \\
    \bottomrule
  \end{tabular}
  }

  \vspace{0.3cm}
  \begin{itemize}
    \item \textbf{Baseline}: precision$_\text{conc}$ = 1.0 ma recall$_\text{conc}$ = 0.372.
          Quando predice \texttt{conc} ha sempre ragione, ma
          \textbf{perde il 63\% delle conclusioni} (le classifica come premesse).
    \item \textbf{BERTino}: F1$_\text{conc}$ = 0.865 vs 0.542
          (\textbf{+32.3 pp}). Il guadagno complessivo di Macro-F1 viene
          quasi interamente da qui.
    \item La classe \texttt{prem} \`{e} gi\`{a} facile per entrambi (F1 $>$ 0.95).
  \end{itemize}
\end{frame}

% ── Slide 16: Perché l'accuracy inganna ──────────────────────────────────────
\begin{frame}{Perch\'{e} l'Accuracy Inganna}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \centering
      \begin{tabular}{lcc}
        \toprule
        & \textbf{Acc.} & \textbf{Macro-F1} \\
        \midrule
        TF-IDF + LR & 91.75\% & 0.748 \\
        BERTino      & 96.46\% & 0.922 \\
        \midrule
        $\Delta$     & +4.71 pp & \textbf{+17.4 pp} \\
        \bottomrule
      \end{tabular}

      \vspace{0.3cm}
      {\small Se guardassimo solo l'accuracy, la baseline sembrerebbe
      \emph{``quasi buona''} (91.75\%).}
    \end{column}
    \begin{column}{0.5\textwidth}
      {\small
      \textbf{Ma cosa succede realmente?}
      \begin{itemize}
        \item La baseline predice \textbf{tutte le 516} premesse correttamente
              (recall$_\text{prem}$ = 100\%)
        \item Ma classifica \textbf{49 conclusioni su 78} come premesse
        \item $\Rightarrow$ Accuracy alta solo perch\'{e} la classe maggioritaria
              (87\%) domina il denominatore
      \end{itemize}

      \vspace{0.15cm}
      \textbf{Macro-F1 rivela la verit\`{a}}: la baseline \`{e} un classificatore
      ``pigro'' che tende a predire \texttt{prem}.
      }
    \end{column}
  \end{columns}

  \vspace{0.15cm}
  {\small $\Rightarrow$ Questo conferma la scelta di Macro-F1 fatta in fase di EDA.}
\end{frame}

% ── Slide 17: Confusion Matrix ──────────────────────────────────────────────
\begin{frame}{Confusion Matrix --- Test Set}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=0.82\textwidth]{../results/plots/cm_baseline_test.png}
      \\{\small \textbf{Baseline} (TF-IDF + LR)}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=0.82\textwidth]{../results/plots/cm_bertino_test.png}
      \\{\small \textbf{BERTino} (fine-tuned)}
    \end{column}
  \end{columns}

  \vspace{0.25cm}
  {\small
  \textbf{Baseline}: 0 falsi positivi (non predice mai \texttt{conc} erroneamente),
  ma \textbf{49 falsi negativi} (63\% delle conclusioni perse).
  \\
  \textbf{BERTino}: solo 21 errori totali (10 FP + 11 FN)
  --- distribuzione \textbf{bilanciata} degli errori.
  \\[0.1cm]
  $\Rightarrow$ BERTino non si limita a ``imparare la classe facile'':
  riesce a distinguere attivamente le conclusioni.}
\end{frame}

% ── Slide 18: Training Dynamics ──────────────────────────────────────────────
\begin{frame}{Training Dynamics --- Convergenza di BERTino}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/training_dynamics.png}
    \end{column}
    \begin{column}{0.38\textwidth}
      \centering
      {\small
      \begin{tabular}{lcc}
        \toprule
        \textbf{Ep.} & \textbf{Val F1} & \textbf{Val Loss} \\
        \midrule
        1 & 0.878 & 0.139 \\
        2 & 0.902 & \textbf{0.129} \\
        3 & \textbf{0.907} & 0.137 \\
        \bottomrule
      \end{tabular}
      }
    \end{column}
  \end{columns}

  \vspace{0.2cm}
  {\small
  \begin{itemize}
    \item Training loss: 0.416 $\to$ 0.079 (convergenza rapida).
    \item Macro-F1 cresce monotonamente: +2.4 pp (ep.\ 1$\to$2), +0.5 pp (ep.\ 2$\to$3).
    \item \textbf{Epoch 3}: la eval loss risale leggermente (0.129 $\to$ 0.137),
          possibile inizio di overfitting, ma F1 migliora ancora.
    \item Best checkpoint selezionato a epoch 3 (F1 = 0.907).
    \item $\Rightarrow$ Con 3 epoche e lr=$2 \times 10^{-5}$ il modello converge
          senza necessit\`{a} di ulteriore tuning.
  \end{itemize}
  }
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                       6. ANALISI DEGLI ERRORI
% ══════════════════════════════════════════════════════════════════════════════
\section{Analisi degli Errori}

% ── Slide 19: Errori di BERTino ──────────────────────────────────────────────
\begin{frame}{Analisi degli Errori di BERTino --- 21 Errori su 594}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \begin{block}{Tipologia degli errori}
        \centering
        \begin{tabular}{lcc}
          \toprule
          \textbf{Tipo} & \textbf{N} & \textbf{Conf.} \\
          \midrule
          prem $\to$ conc (FP) & 10 & 0.797 \\
          conc $\to$ prem (FN) & 11 & 0.907 \\
          \bottomrule
        \end{tabular}
      \end{block}
      \vspace{0.15cm}
      {\small
      \textbf{Osservazione chiave}: gli errori FN
      (conclusioni perse) sono pi\`{u} \textbf{confidenti}
      (0.907 vs 0.797).\\[0.1cm]
      Il modello ``\`{e} convinto'' di sbagliare
      $\Rightarrow$ testi strutturalmente ambigui,
      non incertezza del modello.}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/confidence_analysis.png}
    \end{column}
  \end{columns}
\end{frame}

% ── Slide 20: Lunghezza errori e confronto baseline ──────────────────────────
\begin{frame}{Pattern degli Errori: Lunghezza e Confronto con Baseline}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/error_length_comparison.png}
      \\[0.1cm]
      {\small Errori concentrati su testi corti:\\
      $\mu_\text{errori}$ = 25.9 parole\\
      $\mu_\text{corretti}$ = 47.5 parole\\[0.1cm]
      Testi corti $\Rightarrow$ meno contesto
      $\Rightarrow$ pi\`{u} difficile classificare.}
    \end{column}
    \begin{column}{0.48\textwidth}
      \centering
      \includegraphics[width=0.82\textwidth]{../results/plots/cross_correctness.png}

      \vspace{0.15cm}
      {\small BERTino \textbf{corregge 38/49} errori
      della baseline, ma ne \textbf{introduce 10 nuovi}.\\[0.1cm]
      I 10 nuovi errori sono premesse con
      linguaggio performativo
      (es.\ ``va rigettato'', ``deve essere accolto'')
      --- frasi che \emph{suonano} come conclusioni.}
    \end{column}
  \end{columns}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                        7. ABLATION STUDY
% ══════════════════════════════════════════════════════════════════════════════
\section{Ablation Study}

% ── Slide 21: Ablation baseline ──────────────────────────────────────────────
\begin{frame}{Ablation Study --- Si Pu\`{o} Migliorare la Baseline?}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../results/plots/ablation_baseline.png}
    \end{column}
    \begin{column}{0.43\textwidth}
      \centering
      {\small
      \begin{tabular}{lc}
        \toprule
        \textbf{Configurazione} & \textbf{Test F1} \\
        \midrule
        (1,1) k=10\,000 & \textbf{0.763} \\
        (1,2) k=10\,000 & 0.748 \\
        (1,3) k=10\,000 & 0.756 \\
        (2,2) k=10\,000 & 0.603 \\
        \midrule
        BERTino & \textbf{0.922} \\
        \bottomrule
      \end{tabular}
      }

      \vspace{0.25cm}
      {\small
      La migliore config.\ baseline \`{e}
      \textbf{unigrammi puri} (F1=0.763),
      ma dista ancora \textbf{15.9 pp} da BERTino.\\[0.1cm]
      I bigrammi da soli (2,2) sono
      \textbf{catastrofici}: 0.603.\\[0.1cm]
      $\Rightarrow$ Il gap \`{e} \textbf{strutturale},
      non colmabile con hyperparameter tuning.}
    \end{column}
  \end{columns}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                        8. DISCUSSIONE
% ══════════════════════════════════════════════════════════════════════════════
\section{Discussione}

% ── Slide 22: Perché BERTino vince ───────────────────────────────────────────
\begin{frame}{Perch\'{e} BERTino Vince --- Sintesi Ragionata}
  \textbf{Ricostruiamo il ragionamento dall'EDA ai risultati:}

  \vspace{0.15cm}
  \begin{enumerate}
    \item L'EDA mostra che l'overlap lessicale \`{e} del 7.3\%:
          premesse e conclusioni \emph{condividono quasi tutto il vocabolario}.
    \item TF-IDF, basato su frequenze di termini, cattura solo i segnali
          lessicali forti (``pertanto'', ``appello''), ma non riesce a distinguere
          quando le stesse parole hanno \emph{ruolo argomentativo diverso}.
    \item Di conseguenza, la baseline ``gioca sul sicuro'':
          recall$_\text{prem}$ = 100\%, recall$_\text{conc}$ = 37\%.
    \item BERTino, grazie alla self-attention bidirezionale, modella
          l'\textbf{ordine delle parole} e le \textbf{dipendenze sintattiche}.
    \item Risultato: recall$_\text{conc}$ dal 37\% all'86\%, con un costo
          minimo su \texttt{prem} (recall 100\% $\to$ 98\%).
  \end{enumerate}

  \vspace{0.15cm}
  \begin{colorblock}[white]{sintefblu2}{Conclusione metodologica}
    Il guadagno di +17.4 pp non \`{e} dovuto a ``pi\`{u} parametri'' ma al fatto che
    BERTino cattura \textbf{informazione contestuale} che TF-IDF ignora per costruzione.
  \end{colorblock}
\end{frame}

% ── Slide 23: Errori residui ─────────────────────────────────────────────────
\begin{frame}{Errori Residui --- Cosa Ci Dicono?}
  I 21 errori di BERTino non sono casuali ma rivelano i
  \textbf{limiti intrinseci} del task:

  \vspace{0.2cm}
  \begin{itemize}
    \item \textbf{Testi corti} ($\mu$ = 25.9 parole):
          meno contesto disponibile per la self-attention.
    \item \textbf{FN ad alta confidenza} (0.907):
          conclusioni che \emph{non contengono indicatori} formulaici
          tipici (es.\ manca ``pertanto'', ``per questi motivi'').
    \item \textbf{FP}: premesse che usano linguaggio dispositivo
          (``va rigettato'', ``deve essere accolto'' --- il giudice
          riporta la posizione di una parte).
    \item BERTino corregge \textbf{38/49} errori della baseline,
          ma ne introduce 10 nuovi: casi in cui la pragmatica
          prevale sulla sintassi.
  \end{itemize}

  \vspace{0.15cm}
  {\small $\Rightarrow$ Per superare questi limiti servirebbero
  informazioni \textbf{strutturali} (posizione nel documento, relazioni
  tra componenti) o modelli pi\`{u} grandi.}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                         9. CONCLUSIONI
% ══════════════════════════════════════════════════════════════════════════════
\begin{chapter}[assets/background_negative]{sintefred}{Conclusioni}
\end{chapter}
\section{Conclusioni}

% ── Slide 24: Risultati chiave ───────────────────────────────────────────────
\footlinecolor{sintefred}
\begin{frame}{Risultati Chiave}
  \begin{enumerate}
    \item \textbf{BERTino supera nettamente la baseline}:
          Macro-F1 = 0.922 vs 0.748 (\textbf{+17.4 pp}) sul test set.
    \item Il guadagno principale \`{e} sulla \textbf{classe minoritaria}:
          F1$_\text{conc}$ = 0.865 vs 0.542 (+32.3 pp).
          Recall dal 37\% all'86\%.
    \item L'\textbf{accuracy inganna}: la baseline sembra buona (91.75\%)
          ma Macro-F1 rivela che ignora la maggioranza delle conclusioni.
    \item BERTino \textbf{generalizza bene}: prestazioni stabili tra
          validation (0.907) e test (0.922), a differenza della baseline
          (0.598 $\to$ 0.748).
    \item L'ablation study mostra che il gap \`{e} \textbf{strutturale}:
          nessuna configurazione TF-IDF+LR pu\`{o} colmarlo.
    \item I 21 errori residui sono su \textbf{testi corti e ambigui},
          non su fallimenti sistematici del modello.
  \end{enumerate}
\end{frame}

% ── Slide 25: Limiti ─────────────────────────────────────────────────────────
\begin{frame}{Limiti dello Studio}
  \begin{itemize}
    \item \textbf{Dataset piccolo}: $\sim$3.3k campioni $\to$ varianza alta,
          specialmente sulla classe \texttt{conc} (solo 78 nel test set).
          Un intervallo di confidenza su Macro-F1 sarebbe informativo.
    \item \textbf{Solo Task 1} di AMELIA:
          non abbiamo esplorato le relazioni tra componenti
          (Task 2: relation extraction) n\'{e} la struttura globale
          dell'argomentazione (Task 3).
    \item \textbf{Un solo Transformer}: BERTino \`{e} l'unico modello neurale
          testato.
          Il confronto con UmBERTo, AlBERTo o modelli multilingue
          direbbe se il guadagno dipende dal modello o dalla classe
          di approcci.
    \item \textbf{Nessuna cross-validation}: usiamo split fissi.
          Con 3.3k campioni, $k$-fold aggiungerebbe robustezza.
    \item \textbf{No hyperparameter search} per BERTino: learning rate,
          epoche e batch size usati con valori standard.
  \end{itemize}
\end{frame}

% ── Slide 26: Lavori futuri ──────────────────────────────────────────────────
\begin{frame}{Possibili Estensioni}
  \begin{itemize}
    \item \textbf{Confronto modelli}: UmBERTo, AlBERTo, LEGAL-BERT-IT,
          XLM-RoBERTa --- per isolare il contributo del pre-training italiano.
    \item \textbf{PEFT / LoRA} (cfr.\ lezione su LLM): ridurre i parametri
          aggiornabili mantenendo le prestazioni.
    \item \textbf{Data augmentation}: back-translation, parafrasizzazione
          con LLM per bilanciare la classe \texttt{conc}.
    \item \textbf{Multi-task}: estendere a Task 2--3 di AMELIA
          (relation extraction, struttura argomentativa).
    \item \textbf{Threshold tuning}: calibrare la soglia di classificazione
          per bilanciare precision/recall su \texttt{conc}
          (attualmente 0.870/0.859).
    \item \textbf{Informazioni strutturali}: posizione della componente
          nel documento, lunghezza relativa, componenti adiacenti.
  \end{itemize}
\end{frame}


% ══════════════════════════════════════════════════════════════════════════════
%                         10. RISORSE
% ══════════════════════════════════════════════════════════════════════════════
\section{Risorse}

% ── Slide 27: Engineering ────────────────────────────────────────────────────
\footlinecolor{}
\begin{frame}{Engineering \& Qualit\`{a} del Codice}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \begin{block}{Struttura progetto}
        \begin{itemize}
          \item Pacchetto pip-installable (\texttt{pyproject.toml})
          \item \textbf{39 test unitari} (pytest)
          \item CI/CD: GitHub Actions (ruff + pytest)
          \item Ruff: linting + formatting (100\% clean)
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.48\textwidth}
      \begin{block}{Artefatti prodotti}
        \begin{itemize}
          \item 2 notebook (EDA + Error Analysis)
          \item 14 grafici pubblicabili (300 DPI)
          \item Metriche in JSON + CSV + \LaTeX
          \item README completo in inglese
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}

  \vspace{0.25cm}
  \centering
  \begin{tabular}{ll}
    \textbf{Repository} & \hrefcol{https://github.com/battles5/amelia-bertino-legal-nlp}{github.com/battles5/amelia-bertino-legal-nlp} \\
    \textbf{Dataset} & \hrefcol{https://huggingface.co/datasets/nlp-unibo/AMELIA}{huggingface.co/datasets/nlp-unibo/AMELIA} \\
    \textbf{Modello} & \hrefcol{https://huggingface.co/indigo-ai/BERTino}{huggingface.co/indigo-ai/BERTino} \\
  \end{tabular}
\end{frame}

% ── Bibliografia ─────────────────────────────────────────────────────────────
\begin{frame}[allowframebreaks]{Riferimenti}
  \bibliographystyle{apalike}
  \bibliography{references}
\end{frame}

% ══════════════════════════════════════════════════════════════════════════════
% FINAL SLIDE
% ══════════════════════════════════════════════════════════════════════════════
\backmatter

\end{document}
